{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing scene 000000 ...\n",
      "Parsing scene 000001 ...\n",
      "Parsing scene 000002 ...\n",
      "Parsing scene 000003 ...\n",
      "Parsing scene 000004 ...\n",
      "Parsing scene 000005 ...\n",
      "Parsing scene 000006 ...\n",
      "Parsing scene 000007 ...\n",
      "Parsing scene 000008 ...\n",
      "Parsing scene 000009 ...\n",
      "Parsing scene 000010 ...\n",
      "Parsing scene 000011 ...\n",
      "Parsing scene 000012 ...\n",
      "Parsing scene 000013 ...\n",
      "Parsing scene 000014 ...\n",
      "Parsed scenes: ['000000', '000001', '000002', '000003', '000004', '000005', '000006', '000007', '000008', '000009', '000010', '000011', '000012', '000013', '000014']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_to = \"../..\"\n",
    "\n",
    "# Set your local paths to each folder\n",
    "ipd_val_dir = f\"{path_to}/ipd_val/val\"       # Validation images and labels\n",
    "ipd_base_dir = f\"{path_to}/ipd_base\"     # Base archive: camera parameters, dataset_info, etc.\n",
    "ipd_models_dir = f\"{path_to}/ipd_models\" # 3D object models (PLY files)\n",
    "\n",
    "# Utility to extract image ID from filename (e.g., \"000123.png\" -> \"123\" -> \"123\")\n",
    "def get_image_id(filename):\n",
    "    numeric_str = re.sub(r'\\D', '', filename)\n",
    "    return str(int(numeric_str)) if numeric_str else None\n",
    "\n",
    "def parse_scene_jsons(scene_path):\n",
    "    \"\"\"\n",
    "    Parse the JSON files in a single scene folder.\n",
    "    Returns dictionaries for:\n",
    "      - Camera parameters: cam_params_dict[cam_idx][img_id]\n",
    "      - Ground-truth poses: gt_dict[cam_idx][img_id]\n",
    "      - Ground-truth info (bounding boxes): gt_info_dict[cam_idx][img_id]\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries for cameras 0..9 (adjust if needed)\n",
    "    cam_params_dict = {i: {} for i in range(10)}\n",
    "    gt_dict         = {i: {} for i in range(10)}\n",
    "    gt_info_dict    = {i: {} for i in range(10)}\n",
    "    \n",
    "    json_files = sorted([f for f in os.listdir(scene_path) if f.endswith(\".json\")])\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(scene_path, json_file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # Find camera index in filename, e.g. \"scene_camera_cam1.json\"\n",
    "        match = re.search(r'_cam(\\d+)', json_file)\n",
    "        if not match:\n",
    "            continue\n",
    "        cam_idx = int(match.group(1))\n",
    "        \n",
    "        if \"scene_camera_cam\" in json_file:\n",
    "            for img_id_str, cam_params in data.items():\n",
    "                K = np.array(cam_params[\"cam_K\"]).reshape(3, 3)\n",
    "                R = np.array(cam_params[\"cam_R_w2c\"]).reshape(3, 3)\n",
    "                t = np.array(cam_params[\"cam_t_w2c\"])\n",
    "                depth_scale = cam_params[\"depth_scale\"]\n",
    "                cam_params_dict[cam_idx][img_id_str] = {\n",
    "                    \"cam_K\": K,\n",
    "                    \"cam_R_w2c\": R,\n",
    "                    \"cam_t_w2c\": t,\n",
    "                    \"depth_scale\": depth_scale\n",
    "                }\n",
    "        elif \"scene_gt_cam\" in json_file:\n",
    "            for img_id_str, annotations in data.items():\n",
    "                pose_list = []\n",
    "                for ann in annotations:\n",
    "                    R_m2c = np.array(ann[\"cam_R_m2c\"]).reshape(3, 3)\n",
    "                    t_m2c = np.array(ann[\"cam_t_m2c\"])\n",
    "                    obj_id = ann[\"obj_id\"]\n",
    "                    pose_list.append({\n",
    "                        \"cam_R_m2c\": R_m2c,\n",
    "                        \"cam_t_m2c\": t_m2c,\n",
    "                        \"obj_id\": obj_id\n",
    "                    })\n",
    "                gt_dict[cam_idx][img_id_str] = pose_list\n",
    "        elif \"scene_gt_info_cam\" in json_file:\n",
    "            for img_id_str, info_list in data.items():\n",
    "                gt_info_dict[cam_idx][img_id_str] = info_list\n",
    "    \n",
    "    return {\n",
    "        \"cam_params\": cam_params_dict,\n",
    "        \"gt\": gt_dict,\n",
    "        \"gt_info\": gt_info_dict\n",
    "    }\n",
    "\n",
    "def parse_all_scenes(val_path):\n",
    "    \"\"\"\n",
    "    Iterate over all scene folders in val_path (e.g. \"000000\", \"000001\", ...),\n",
    "    parse their JSON files, and return a dict keyed by scene ID.\n",
    "    \"\"\"\n",
    "    scene_dict = {}\n",
    "    scene_folders = sorted([d for d in os.listdir(val_path) if d.isdigit()])\n",
    "    for scene_id in scene_folders:\n",
    "        scene_folder = os.path.join(val_path, scene_id)\n",
    "        if not os.path.isdir(scene_folder):\n",
    "            continue\n",
    "        print(f\"Parsing scene {scene_id} ...\")\n",
    "        scene_data = parse_scene_jsons(scene_folder)\n",
    "        scene_dict[scene_id] = scene_data\n",
    "    return scene_dict\n",
    "\n",
    "# Parse all scenes from the validation directory (assumed to contain scene folders)\n",
    "all_scenes = parse_all_scenes(f\"{ipd_val_dir}\")\n",
    "print(\"Parsed scenes:\", list(all_scenes.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DualModalDataset(Dataset):\n",
    "    def __init__(self, scene_dict, scene_id, cam_idx, val_folder,\n",
    "                 rgb_folder_template=\"rgb_cam{}\", depth_folder_template=\"depth_cam{}\",\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scene_dict (dict): Parsed scenes dictionary.\n",
    "            scene_id (str): Scene ID (e.g., \"000000\").\n",
    "            cam_idx (int): Camera index to use (here, we use 1).\n",
    "            rgb_folder_template (str): Template for the RGB folder name.\n",
    "            depth_folder_template (str): Template for the depth folder name.\n",
    "            transform (callable, optional): Optional transform to apply on images.\n",
    "        \"\"\"\n",
    "        self.scene_data = scene_dict[scene_id]\n",
    "        self.cam_idx = cam_idx\n",
    "        self.rgb_folder = os.path.join(val_folder, scene_id, rgb_folder_template.format(cam_idx))\n",
    "        self.depth_folder = os.path.join(val_folder, scene_id, depth_folder_template.format(cam_idx))\n",
    "        self.image_files = sorted([f for f in os.listdir(self.rgb_folder) if f.endswith((\".png\", \".jpg\"))])\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load RGB image\n",
    "        rgb_file = self.image_files[idx]\n",
    "        rgb_path = os.path.join(self.rgb_folder, rgb_file)\n",
    "        rgb_img = cv2.imread(rgb_path)\n",
    "        if rgb_img is None:\n",
    "            raise ValueError(f\"Failed to load RGB image: {rgb_path}\")\n",
    "        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load corresponding depth image (assumes same filename in depth folder)\n",
    "        depth_path = os.path.join(self.depth_folder, rgb_file)\n",
    "        depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)  # depth may be single-channel\n",
    "        if depth_img is None:\n",
    "            raise ValueError(f\"Failed to load Depth image: {depth_path}\")\n",
    "        # If depth image has more than one channel, take the first channel\n",
    "        if len(depth_img.shape) == 3:\n",
    "            depth_img = depth_img[:, :, 0]\n",
    "        \n",
    "        # Normalize images:\n",
    "        rgb_img = rgb_img.astype(np.float32) / 255.0  # RGB in [0,1]\n",
    "        depth_img = depth_img.astype(np.float32)\n",
    "        # Optionally, scale depth image (using a depth scale from camera parameters could be used here)\n",
    "        depth_img = depth_img / np.max(depth_img) if np.max(depth_img) > 0 else depth_img\n",
    "        \n",
    "        # Create a dummy segmentation mask from bounding boxes\n",
    "        # Initialize mask with zeros (binary segmentation)\n",
    "        mask = np.zeros((rgb_img.shape[0], rgb_img.shape[1]), dtype=np.uint8)\n",
    "        img_id = str(int(get_image_id(rgb_file)))\n",
    "        gt_info = self.scene_data[\"gt_info\"].get(self.cam_idx, {}).get(img_id, [])\n",
    "        for obj in gt_info:\n",
    "            if \"bbox_visib\" in obj:\n",
    "                x, y, w, h = obj[\"bbox_visib\"]\n",
    "            elif \"bbox_obj\" in obj:\n",
    "                x, y, w, h = obj[\"bbox_obj\"]\n",
    "            else:\n",
    "                continue\n",
    "            x, y, w, h = map(int, [x, y, w, h])\n",
    "            cv2.rectangle(mask, (x, y), (x+w, y+h), 1, -1)  # fill rectangle with 1\n",
    "        \n",
    "        # Convert images and mask to torch tensors\n",
    "        rgb_tensor = torch.from_numpy(rgb_img).permute(2, 0, 1)  # (3, H, W)\n",
    "        depth_tensor = torch.from_numpy(depth_img).unsqueeze(0)    # (1, H, W)\n",
    "        mask_tensor = torch.from_numpy(mask).long()                # (H, W) with class labels {0,1}\n",
    "        \n",
    "        if self.transform:\n",
    "            rgb_tensor = self.transform(rgb_tensor)\n",
    "            depth_tensor = self.transform(depth_tensor)\n",
    "        \n",
    "        # Return a tuple: (rgb, depth, segmentation mask)\n",
    "        return rgb_tensor, depth_tensor, mask_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FuseNetDual(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FuseNetDual, self).__init__()\n",
    "        # RGB encoder branch (input: 3 channels)\n",
    "        self.rgb_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.rgb_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Depth encoder branch (input: 1 channel)\n",
    "        self.depth_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.depth_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Fusion: concatenate features from both branches\n",
    "        # After encoder, assume RGB branch gives 32 channels and depth branch gives 16 channels\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(32+16, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Decoder: simple upsampling to original resolution\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, rgb, depth):\n",
    "        # Encode RGB\n",
    "        x_rgb = self.rgb_conv1(rgb)    # (B, 16, H/2, W/2)\n",
    "        x_rgb = self.rgb_conv2(x_rgb)   # (B, 32, H/4, W/4)\n",
    "        # Encode depth\n",
    "        x_depth = self.depth_conv1(depth)  # (B, 8, H/2, W/2)\n",
    "        x_depth = self.depth_conv2(x_depth)  # (B, 16, H/4, W/4)\n",
    "        # Fuse: concatenate along channel dimension\n",
    "        x = torch.cat([x_rgb, x_depth], dim=1)  # (B, 48, H/4, W/4)\n",
    "        x = self.fusion_conv(x)                 # (B, 64, H/4, W/4)\n",
    "        # Decode back to segmentation map (assume num_classes channels)\n",
    "        x = self.decoder(x)  # (B, num_classes, H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Configuration dictionary for hyperparameters and training\n",
    "config = {\n",
    "    \"num_classes\": 2,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_epochs\": 5,\n",
    "    \"batch_size\": 4,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = FuseNetDual(num_classes=config[\"num_classes\"]).to(config[\"device\"])\n",
    "print(\"Model initialized on device:\", config[\"device\"])\n",
    "\n",
    "# Loss: CrossEntropyLoss for segmentation (target shape: (B, H, W) with class labels)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [4, 2, 4320, 7680] , target: [4, 2160, 3840]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(rgb, depth)  \u001b[38;5;66;03m# outputs: (B, num_classes, H, W)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# CrossEntropyLoss expects raw logits and target of shape (B, H, W)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch (got input: [4, 2, 4320, 7680] , target: [4, 2160, 3840]"
     ]
    }
   ],
   "source": [
    "# Training Loop on the Validation Dataset\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (rgb, depth, target) in enumerate(dataloader):\n",
    "            rgb = rgb.to(device)\n",
    "            depth = depth.to(device)\n",
    "            # For CrossEntropyLoss, target should be of shape (B, H, W) and of type long.\n",
    "            target = target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rgb, depth)  # outputs: (B, num_classes, H, W)\n",
    "            # CrossEntropyLoss expects raw logits and target of shape (B, H, W)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * rgb.size(0)\n",
    "        \n",
    "        avg_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "    return epoch_losses\n",
    "\n",
    "scene_id = \"000000\"\n",
    "cam_idx = 1\n",
    "\n",
    "# Create dataset and dataloader for dual-modal data\n",
    "dataset = DualModalDataset(all_scenes, scene_id, cam_idx, ipd_val_dir,\n",
    "                           rgb_folder_template=\"rgbloss_cam{}\", depth_folder_template=\"depth_cam{}\")\n",
    "dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "\n",
    "# Train the model\n",
    "epoch_losses = train_model(model, dataloader, criterion, optimizer, config[\"num_epochs\"], config[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, config[\"num_epochs\"]+1), epoch_losses, marker=\"o\", linestyle=\"--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Averaged Training Loss Across Epochs\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
