{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1080\n",
      "Torch version: 2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root folder to path (assuming /Notebooks/ is in /DL_Final_Project/)\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device CPU\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../trans_loss_4y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m extension \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4y\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load both JSON files.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../trans_loss_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mextension\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m     trans_losses \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../rot_loss_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../trans_loss_4y'"
     ]
    }
   ],
   "source": [
    "# # Cell 1: Visualize training losses\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# print(\"Device\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"CPU\"))\n",
    "\n",
    "# extension = '4y'\n",
    "\n",
    "# # Load both JSON files.\n",
    "# with open(f\"../trans_loss_{extension}\", \"r\") as f:\n",
    "#     trans_losses = json.load(f)\n",
    "\n",
    "# with open(f\"../rot_loss_{extension}\", \"r\") as f:\n",
    "#     rot_losses = json.load(f)\n",
    "\n",
    "# episodes = np.arange(1, len(trans_losses) + 1)\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(rot_losses, label=\"Rotation Loss\", color=\"orange\")\n",
    "# plt.plot(trans_losses, label=\"Translation Loss\", color=\"blue\")\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training Loss Curves\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import yaml\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " # --- Config ---\n",
    "\n",
    "from Classes.Dataset.IPDDataset_render import IPDDatasetMounted\n",
    "from Models.PoseEstimator.PoseEstimation import PoseEstimator\n",
    "\n",
    "\n",
    "config_path = \"../Config/config_fusenet.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "remote_base_url = f\"../{config['dataset']['remote_base_url']}\"\n",
    "cam_ids = config[\"dataset\"][\"cam_ids\"]\n",
    "modalities = config[\"dataset\"].get(\"modality\", [\"rgb\", \"depth\"])\n",
    "\n",
    "# For seen CAD models, use the training split and same allowed objects as in training.\n",
    "train_scene_ids = {f\"{i:06d}\" for i in range(0, 25)}\n",
    "train_obj_ids = {0, 8, 18, 19, 20}\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "\n",
    "dataset = IPDDatasetMounted(\n",
    "    remote_base_url,\n",
    "    cam_ids,\n",
    "    modalities,\n",
    "    split=config[\"dataset\"].get(\"val_split\", \"train\"),  # using training split\n",
    "    # allowed_scene_ids=train_scene_ids,\n",
    "    # allowed_obj_ids=train_obj_ids\n",
    ")\n",
    "\n",
    "sensory_channels = {mod: 1 for mod in modalities}\n",
    "encoder_type = config[\"training\"].get(\"encoder\", \"fusenet\")\n",
    "fusion_type = config[\"training\"].get(\"fusion\", \"concat\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "renderer_config = config.get(\"renderer\", {\"width\": 640, \"height\": 480, \"device\": device})\n",
    "\n",
    "model = PoseEstimator(\n",
    "    sensory_channels, renderer_config,\n",
    "    encoder_type=encoder_type,\n",
    "    fusion_type=fusion_type,\n",
    "    n_views=len(cam_ids)\n",
    ").to(device)\n",
    "\n",
    "model_path = f\"../weights/model_{encoder_type}_{fusion_type}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_sample_to_device(sample, device):\n",
    "    X, Y = sample[\"X\"], sample[\"Y\"]\n",
    "    # Move each view data to the device\n",
    "    for view in X[\"views\"]:\n",
    "        for k in view:\n",
    "            view[k] = view[k].to(device)\n",
    "    X[\"K\"] = X[\"K\"].to(device)\n",
    "    # Move CAD model tensors\n",
    "    for item in X[\"available_cads\"]:\n",
    "        item[\"verts\"] = item[\"verts\"].to(device)\n",
    "        item[\"faces\"] = item[\"faces\"].to(device)\n",
    "    # Move ground-truth pose tensors\n",
    "    for pose in Y[\"gt_poses\"]:\n",
    "        pose[\"R\"] = pose[\"R\"].to(device)\n",
    "        pose[\"t\"] = pose[\"t\"].to(device)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from ../weights/model_fusenet_concat.pt...\n",
      "Loading model weights...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- Load a sample from dataset ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Assuming dataset is defined somewhere (e.g., `dataset = IPDDatasetMounted(...)`)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m sample \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# Choose a sample index\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mmove_sample_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m], sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Count the number of ground truth poses per sample\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m, in \u001b[0;36mmove_sample_to_device\u001b[1;34m(sample, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_sample_to_device\u001b[39m(sample, device):\n\u001b[1;32m----> 2\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Move each view data to the device\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m X[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviews\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Models.helpers import quaternion_to_matrix\n",
    "from Models.PoseEstimator.PoseEstimation import PoseEstimator\n",
    "from Metrics.visualization import draw_pose_axes, draw_bbox_from_pose\n",
    "from Models.KaolinRenderer import KaolinRenderer\n",
    "\n",
    "# Load model weights (using your predefined path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Loading model weights from {model_path}...\")\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model weights...\")\n",
    "    # model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# --- Load a sample from dataset ---\n",
    "# Assuming dataset is defined somewhere (e.g., `dataset = IPDDatasetMounted(...)`)\n",
    "sample = dataset[4]  # Choose a sample index\n",
    "sample = move_sample_to_device(sample, device)\n",
    "X, Y = sample[\"X\"], sample[\"Y\"]\n",
    "\n",
    "# Count the number of ground truth poses per sample\n",
    "gt_poses = Y[\"gt_poses\"]\n",
    "\n",
    "# Build a lookup table for available CAD models (seen during training)\n",
    "cad_model_lookup = {\n",
    "    item[\"obj_id\"]: {\"verts\": item[\"verts\"], \"faces\": item[\"faces\"]}\n",
    "    for item in X[\"available_cads\"]\n",
    "}\n",
    "\n",
    "# Perform inference to get detections and computed poses\n",
    "# with torch.no_grad():\n",
    "    # detections, _, _ = model(\n",
    "    #     x_dict_views=X[\"views\"],\n",
    "    #     K_list=X[\"K\"],\n",
    "    #     cad_model_lookup=cad_model_lookup\n",
    "    # )\n",
    "\n",
    "# --- Visualize prediction vs ground truth ---\n",
    "# Choose the render mode: \"axes\" or \"bbox\"\n",
    "render_mode = \"bbox\"\n",
    "\n",
    "# Assume the camera intrinsics (K) is stored in X[\"K\"][0]\n",
    "K = X[\"K\"][0].cpu().numpy()\n",
    "\n",
    "# Get the number of views available in X[\"views\"]\n",
    "num_views = len(X[\"views\"])\n",
    "\n",
    "# Create subplots to display all views side by side\n",
    "fig, axes = plt.subplots(1, num_views, figsize=(5 * num_views, 5))\n",
    "# If there is only one view, wrap axes in a list\n",
    "if num_views == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Loop through each view and render predictions on the image\n",
    "for idx, view in enumerate(X[\"views\"]):\n",
    "    # Convert the image tensor to a PIL image, then to a NumPy array in RGB\n",
    "    tensor = view[\"rgb\"][0].cpu()\n",
    "    img_pil = ToPILImage()(tensor)\n",
    "    img_rgb = np.array(img_pil.convert(\"RGB\"))\n",
    "    \n",
    "    # Copy image to draw on it\n",
    "    vis = img_rgb.copy()\n",
    "\n",
    "    # Draw predicted poses on each view\n",
    "    for i, det in enumerate(detections):\n",
    "        # Get the predicted pose (convert quaternion to rotation matrix)\n",
    "        R_pred = quaternion_to_matrix(det[\"quat\"].unsqueeze(0))[0].cpu().numpy()\n",
    "        t_pred = det[\"trans\"].cpu().numpy()\n",
    "        \n",
    "        if render_mode == \"axes\":\n",
    "            vis = draw_pose_axes(vis, R_pred, t_pred, K, label=f\"Pred_{i}\", color=(0, 255, 0))\n",
    "        elif render_mode == \"bbox\":\n",
    "            vis = draw_bbox_from_pose(vis, R_pred, t_pred, K, label=f\"Pred_{i}\", color=(0, 255, 0))\n",
    "    \n",
    "    # Optionally, add ground truth information here if needed\n",
    "\n",
    "    axes[idx].imshow(vis)\n",
    "    axes[idx].set_title(f\"View {idx}\")\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
