{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1080\n",
      "Torch version: 2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root folder to path (assuming /Notebooks/ is in /DL_Final_Project/)\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 1: Visualize training losses\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# print(\"Device\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"CPU\"))\n",
    "\n",
    "# extension = '4y'\n",
    "\n",
    "# # Load both JSON files.\n",
    "# with open(f\"../trans_loss_{extension}\", \"r\") as f:\n",
    "#     trans_losses = json.load(f)\n",
    "\n",
    "# with open(f\"../rot_loss_{extension}\", \"r\") as f:\n",
    "#     rot_losses = json.load(f)\n",
    "\n",
    "# episodes = np.arange(1, len(trans_losses) + 1)\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(rot_losses, label=\"Rotation Loss\", color=\"orange\")\n",
    "# plt.plot(trans_losses, label=\"Translation Loss\", color=\"blue\")\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training Loss Curves\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import yaml\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " # --- Config ---\n",
    "\n",
    "from Classes.Dataset.IPDDataset_render import IPDDatasetMounted\n",
    "from Models.PoseEstimatorSeen.PoseEstimation import PoseEstimator\n",
    "\n",
    "\n",
    "config_path = \"../Config/config_fusenet.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "remote_base_url = f\"../{config['dataset']['remote_base_url']}\"\n",
    "cam_ids = config[\"dataset\"][\"cam_ids\"]\n",
    "modalities = config[\"dataset\"].get(\"modality\", [\"rgb\", \"depth\"])\n",
    "\n",
    "# For seen CAD models, use the training split and same allowed objects as in training.\n",
    "train_scene_ids = {f\"{i:06d}\" for i in range(0, 25)}\n",
    "train_obj_ids = {0, 8, 18, 19, 20}\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "\n",
    "dataset = IPDDatasetMounted(\n",
    "    remote_base_url,\n",
    "    cam_ids,\n",
    "    modalities,\n",
    "    split=config[\"dataset\"].get(\"train_split\", \"train\"),  # using training split\n",
    "    allowed_scene_ids=train_scene_ids,\n",
    "    allowed_obj_ids=train_obj_ids\n",
    ")\n",
    "\n",
    "sensory_channels = {mod: 1 for mod in modalities}\n",
    "encoder_type = config[\"training\"].get(\"encoder\", \"fusenet\")\n",
    "fusion_type = config[\"training\"].get(\"fusion\", \"concat\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "renderer_config = config.get(\"renderer\", {\"width\": 640, \"height\": 480, \"device\": device})\n",
    "\n",
    "model = PoseEstimator(\n",
    "    sensory_channels,\n",
    "    encoder_type=encoder_type,\n",
    "    fusion_type=fusion_type,\n",
    "    n_views=len(cam_ids),\n",
    "    num_classes=len(train_obj_ids)  # ðŸ”¥ Add this line to match training\n",
    ").to(device)\n",
    "\n",
    "model_path = f\"../weights/pose_model_{encoder_type}_{fusion_type}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_sample_to_device(sample, device):\n",
    "    X, Y = sample[\"X\"], sample[\"Y\"]\n",
    "\n",
    "    for view in X[\"views\"]:\n",
    "        for k in view:\n",
    "            view[k] = view[k].to(device)\n",
    "\n",
    "    X[\"K\"] = X[\"K\"].to(device)\n",
    "\n",
    "    for pose in Y[\"gt_poses\"]:\n",
    "        pose[\"R\"] = pose[\"R\"].to(device)\n",
    "        pose[\"t\"] = pose[\"t\"].to(device)\n",
    "        pose[\"obj_id\"] = pose[\"obj_id\"].to(device)\n",
    "\n",
    "    for obj_id in X[\"model_points_by_id\"]:\n",
    "        X[\"model_points_by_id\"][obj_id] = X[\"model_points_by_id\"][obj_id].to(device)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from ../weights/pose_model_fusenet_concat.pt...\n",
      "Loading model weights...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PoseEstimator:\n\tsize mismatch for det_head.head.weight: copying a param with shape torch.Size([39, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([54, 384, 1, 1]).\n\tsize mismatch for det_head.head.bias: copying a param with shape torch.Size([39]) from checkpoint, the shape in current model is torch.Size([54]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] Model path not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RoseP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PoseEstimator:\n\tsize mismatch for det_head.head.weight: copying a param with shape torch.Size([39, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([54, 384, 1, 1]).\n\tsize mismatch for det_head.head.bias: copying a param with shape torch.Size([39]) from checkpoint, the shape in current model is torch.Size([54])."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Models.helpers import quaternion_to_matrix\n",
    "from Models.PoseEstimatorSeen.PoseEstimation import PoseEstimator\n",
    "from Metrics.visualization import draw_pose_axes, draw_bbox_from_pose\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model weights\n",
    "print(f\"Loading model weights from {model_path}...\")\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model weights...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "else:\n",
    "    print(f\"[ERROR] Model path not found: {model_path}\")\n",
    "model.eval()\n",
    "\n",
    "# --- Load a sample from dataset ---\n",
    "sample = dataset[1]\n",
    "sample = move_sample_to_device(sample, device)\n",
    "X, Y = sample[\"X\"], sample[\"Y\"]\n",
    "K_list = X[\"K\"]\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    detections, _ = model(\n",
    "        x_dict_views=X[\"views\"],\n",
    "        K_list=K_list,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "# Choose the render mode: \"axes\" or \"bbox\"\n",
    "render_mode = \"bbox\"\n",
    "\n",
    "# Get number of views\n",
    "num_views = len(X[\"views\"])\n",
    "fig, axes = plt.subplots(1, num_views, figsize=(6 * num_views, 6))\n",
    "if num_views == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, view in enumerate(X[\"views\"]):\n",
    "    img_tensor = view[\"rgb\"].cpu()\n",
    "    img_pil = ToPILImage()(img_tensor)\n",
    "    img_rgb = np.array(img_pil.convert(\"RGB\"))\n",
    "    vis = img_rgb.copy()\n",
    "\n",
    "    K_np = K_list[idx].cpu().numpy()\n",
    "\n",
    "    # Draw GT poses in red\n",
    "    for i, gt in enumerate(Y[\"gt_poses\"]):\n",
    "        R_gt = gt[\"R\"].cpu().numpy()\n",
    "        t_gt = gt[\"t\"].cpu().numpy()\n",
    "        if render_mode == \"axes\":\n",
    "            vis = draw_pose_axes(vis, R_gt, t_gt, K_np, label=f\"GT_{i}\", color=(255, 0, 0))\n",
    "        else:\n",
    "            vis = draw_bbox_from_pose(vis, R_gt, t_gt, K_np, label=f\"GT_{i}\", color=(255, 0, 0))\n",
    "\n",
    "    # Draw predictions in green\n",
    "    for i, det in enumerate(detections):\n",
    "        R_pred = quaternion_to_matrix(det[\"quat\"].unsqueeze(0))[0].cpu().numpy()\n",
    "        t_pred = det[\"trans\"].cpu().numpy()\n",
    "        if render_mode == \"axes\":\n",
    "            vis = draw_pose_axes(vis, R_pred, t_pred, K_np, label=f\"Pred_{i}\", color=(0, 255, 0))\n",
    "        else:\n",
    "            vis = draw_bbox_from_pose(vis, R_pred, t_pred, K_np, label=f\"Pred_{i}\", color=(0, 255, 0))\n",
    "\n",
    "    axes[idx].imshow(vis)\n",
    "    axes[idx].set_title(f\"View {idx}\")\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
